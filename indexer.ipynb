{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dbpedia Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yohannes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import elasticsearch\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "from typing import Dict\n",
    "\n",
    "# path variables, etc.\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug mode\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch version: (7, 17, 6)\n",
      "Index smart_index\n",
      "Index settings:\n",
      "{'mappings': {'properties': {'abstract': {'analyzer': 'english',\n",
      "                                          'term_vector': 'yes',\n",
      "                                          'type': 'text'},\n",
      "                             'instance_type': {'type': 'text'}}}}\n",
      "Files to index: c:\\Users\\Yohannes\\Documents\\School\\Master\\DAT640-FinalProject\\datasets\\dbpedia\\short_abstracts_en.ttl c:\\Users\\Yohannes\\Documents\\School\\Master\\DAT640-FinalProject\\datasets\\dbpedia\\instance_types_en.ttl\n"
     ]
    }
   ],
   "source": [
    "print(\"Elasticsearch version:\", elasticsearch.__version__)\n",
    "print(\"Index\", INDEX_NAME)\n",
    "print(\"Index settings:\")\n",
    "pprint(INDEX_SETTINGS)\n",
    "\n",
    "print(\"Files to index:\", SHORT_ABSTRACT_PATH, INSTANCE_TYPES_EN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DbPedia Indexing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbPediaCollection:\n",
    "    def __init__(self, index_name: str, index_settings: Dict, stop_words=[], DEBUG=False, FILES=[]) -> None:\n",
    "        self._index_name = index_name\n",
    "        self._index_settings = index_settings\n",
    "        self.es = Elasticsearch()\n",
    "        self.stop_words = stop_words\n",
    "        self.FILES = FILES\n",
    "\n",
    "        # for local dev\n",
    "        self.DEBUG = DEBUG\n",
    "        self.DEBUG_DATA = set()\n",
    "    \n",
    "    def preprocess(self, line, remove_stopwords=False):\n",
    "        line = line.strip().lower().replace(\"_\", \" \").translate(str.maketrans('', '', string.punctuation))\n",
    "        return \" \".join([\n",
    "            term \n",
    "            for term in re.sub(r\"\\s+\", \" \", line).split(\" \") \n",
    "            if term not in self.stop_words\n",
    "        ]).strip() if remove_stopwords else line\n",
    "\n",
    "    def parse_instance_types(self, line):\n",
    "        if line == None or line[0] == \"#\":\n",
    "            return\n",
    "        \n",
    "        line = line.strip().replace('/>', '>').split(' ')\n",
    "        if len(line) < 3:\n",
    "            return\n",
    "        entity = self.preprocess(line[0][1:-1].split(\"/\")[-1]) # remove < and >, get entity + preprocess\n",
    "        instance_type = self.preprocess(line[2][1:-1].split(\"/\")[-1][4:]) # remove < and >, get instance type + preprocess\n",
    "        return {\n",
    "            \"_id\": entity,\n",
    "            # \"_source\": {\"doc\" : {\"instance_type\": instance_type}},\n",
    "            \"doc\" : {\"instance_type\": instance_type},\n",
    "            \"_op_type\": \"update\"\n",
    "        }\n",
    "    \n",
    "    def parse_abstracts(self, line):\n",
    "        if line == None or line[0] == \"#\":\n",
    "            return\n",
    "        \n",
    "        line = line.strip().replace('@en .', '') \\\n",
    "            .replace('\"', '').replace('\\\\', '') \\\n",
    "            .replace('\\'', '').replace('/>', '>').split(' ')\n",
    "\n",
    "        if len(line) < 3:\n",
    "            return\n",
    "        entity = self.preprocess(line[0][1:-1].split(\"/\")[-1])\n",
    "        abstract = self.preprocess(' '.join(line[2:]), True)\n",
    "\n",
    "        if self.DEBUG and entity not in self.DEBUG_DATA:\n",
    "            self.DEBUG_DATA.add(entity) # TODO fix later\n",
    "\n",
    "        return {\n",
    "            \"_id\": entity,\n",
    "            \"_source\" : {\"abstract\": abstract, \"instance_type\": \"_\"}\n",
    "        }\n",
    "    \n",
    "    def getBulkedData(self, entities):\n",
    "        if self.DEBUG:\n",
    "            return [e for e in entities if e[\"_id\"] in self.DEBUG_DATA]\n",
    "        return entities\n",
    "    \n",
    "    def create_index(self, recreate_index=False):\n",
    "        if self.es.indices.exists(self._index_name):\n",
    "            if recreate_index:\n",
    "                self.es.indices.delete(index=self._index_name)\n",
    "            return\n",
    "        self.es.indices.create(index=self._index_name, body=self._index_settings)\n",
    "    \n",
    "    def query(self, body, size=10):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            res = self.es.search(index=self._index_name, body=body, size=size)\n",
    "            print(\"Query time: {:4f} seconds\".format(time.time() - start_time))\n",
    "            return res\n",
    "        except exceptions.RequestError as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    def index(self, bulk_size=1000, override_debug=False):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            for file in self.FILES:\n",
    "                count = 0\n",
    "                with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    entities = []\n",
    "                    for i, line in enumerate(tqdm(f)):\n",
    "                        if i == 0:\n",
    "                            continue\n",
    "\n",
    "                        if file == SHORT_ABSTRACT_PATH:\n",
    "                           entities.append(self.parse_abstracts(line))\n",
    "                        elif file == INSTANCE_TYPES_EN_PATH:\n",
    "                            entities.append(self.parse_instance_types(line))\n",
    "                        else:\n",
    "                            print(\"Unknown file\", file)\n",
    "                            break\n",
    "                        \n",
    "                        if i % bulk_size == 0: # bulk insert\n",
    "                            helpers.bulk(self.es, self.getBulkedData(entities), index=self._index_name, raise_on_error=False)\n",
    "                            entities = []\n",
    "                            if self.DEBUG:\n",
    "                                break\n",
    "                    if len(entities) > 0:\n",
    "                        helpers.bulk(self.es, self.getBulkedData(entities), index=self._index_name, raise_on_error=False)\n",
    "                        entities = []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            print(\"Indexing finished, time elapsed: {:4f} seconds\".format(time.time() - start_time))\n",
    "            entities, self.DEBUG_DATA = [], set() # reset data\n",
    "        \n",
    "    \n",
    "    \n",
    "    # factory method for creating dbpedia collection\n",
    "    @classmethod\n",
    "    def create_dbpedia_collection(cls):\n",
    "        return cls(INDEX_NAME, INDEX_SETTINGS, stop_words=stopwords.words('english'), DEBUG=DEBUG, FILES=[SHORT_ABSTRACT_PATH, INSTANCE_TYPES_EN_PATH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yohannes\\AppData\\Local\\Temp\\ipykernel_24320\\2379185113.py:64: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  if self.es.indices.exists(self._index_name):\n",
      "c:\\Users\\Yohannes\\anaconda3\\envs\\DAT640\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "dbpedia_index = DbPediaCollection.create_dbpedia_collection()\n",
    "dbpedia_index.create_index(recreate_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147502it [00:59, 2498.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing finished, time elapsed: 59.057037 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdbpedia_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [12], line 88\u001b[0m, in \u001b[0;36mDbPediaCollection.index\u001b[1;34m(self, bulk_size, override_debug)\u001b[0m\n\u001b[0;32m     86\u001b[0m entities \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(f)):\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     90\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yohannes\\anaconda3\\envs\\DAT640\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yohannes\\anaconda3\\envs\\DAT640\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dbpedia_index.index(bulk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4925720\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", dbpedia_index.es.count(index=INDEX_NAME)[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DAT640')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a534e95c56c9f35ffee0f4f6a76cc4d9d6e49a069deac2725bb950cdc0166fc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
