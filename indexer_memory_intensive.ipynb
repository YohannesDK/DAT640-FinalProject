{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dbpedia Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import elasticsearch\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import deque\n",
    "import traceback\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "from typing import Dict\n",
    "\n",
    "# path variables, etc.\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug mode\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch version: (8, 5, 0)\n",
      "Index smart_index\n",
      "Index settings:\n",
      "{'mappings': {'properties': {'abstract': {'analyzer': 'english',\n",
      "                                          'term_vector': 'yes',\n",
      "                                          'type': 'text'},\n",
      "                             'instance_type': {'type': 'text'}}}}\n",
      "Files to index: /mntnvme/datasets/DBpedia/short_abstracts_en.ttl /mntnvme/datasets/DBpedia/instance_types_en.ttl\n"
     ]
    }
   ],
   "source": [
    "print(\"Elasticsearch version:\", elasticsearch.__version__)\n",
    "print(\"Index\", INDEX_NAME)\n",
    "print(\"Index settings:\")\n",
    "pprint(INDEX_SETTINGS)\n",
    "\n",
    "print(\"Files to index:\", SHORT_ABSTRACT_PATH, INSTANCE_TYPES_EN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DbPedia Indexing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbPediaCollection:\n",
    "    def __init__(self, index_name: str, index_settings: Dict, stop_words=[], DEBUG=False, FILES=[]) -> None:\n",
    "        self._index_name = index_name\n",
    "        self._index_settings = index_settings\n",
    "        self.es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "        self.stop_words = stop_words\n",
    "        self.FILES = FILES\n",
    "\n",
    "        # for local dev\n",
    "        self.DEBUG = DEBUG\n",
    "    \n",
    "    def preprocess(self, line, remove_stopwords=False):\n",
    "        line = line.strip().lower().replace(\"_\", \" \").translate(str.maketrans('', '', string.punctuation))\n",
    "        return \" \".join([\n",
    "            term \n",
    "            for term in re.sub(r\"\\s+\", \" \", line).split(\" \") \n",
    "            if term not in self.stop_words\n",
    "        ]).strip() if remove_stopwords else line\n",
    "\n",
    "    def parse_instance_types(self, line):\n",
    "        if line == None or line[0] == \"#\":\n",
    "            return\n",
    "        \n",
    "        line = line.strip().replace('/>', '>').split(' ')\n",
    "        if len(line) < 3:\n",
    "            return\n",
    "        entity = self.preprocess(line[0][1:-1].split(\"/\")[-1]) # remove < and >, get entity + preprocess\n",
    "        instance_type = self.preprocess(line[2][1:-1].split(\"/\")[-1].replace(\"owl#\", \"\")) # remove < and >, get instance type + preprocess\n",
    "        return {\n",
    "            \"_id\": entity,\n",
    "            \"doc\" : {\"instance_type\": instance_type},\n",
    "            \"_op_type\": \"update\"\n",
    "        }\n",
    "    \n",
    "    def parse_abstracts(self, line):\n",
    "        if line == None or line[0] == \"#\":\n",
    "            return\n",
    "        \n",
    "        line = line.strip().replace('@en .', '') \\\n",
    "            .replace('\"', '').replace('\\\\', '') \\\n",
    "            .replace('\\'', '').replace('/>', '>').split(' ')\n",
    "\n",
    "        if len(line) < 3:\n",
    "            return\n",
    "        entity = self.preprocess(line[0][1:-1].split(\"/\")[-1])\n",
    "        abstract = self.preprocess(' '.join(line[2:]), True)\n",
    "\n",
    "        return {\n",
    "            \"_id\": entity,\n",
    "            \"_source\" : {\"abstract\": abstract, \"instance_type\": \"_\"}\n",
    "        }\n",
    "    \n",
    "    def create_index(self, recreate_index=False):\n",
    "        if self.es.indices.exists(index=self._index_name):\n",
    "            if recreate_index:\n",
    "                self.es.indices.delete(index=self._index_name)\n",
    "        self.es.indices.create(index=self._index_name, body=self._index_settings)\n",
    "    \n",
    "    def query(self, body, size=10):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            res = self.es.search(index=self._index_name, body=body, size=size)\n",
    "            print(\"Query time: {:4f} seconds\".format(time.time() - start_time))\n",
    "            return res\n",
    "        except exceptions.RequestError as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    def index(self, bulk_size=1000, override_debug=False):\n",
    "        try:\n",
    "            data = {}\n",
    "            start_time = time.time()\n",
    "            for file in self.FILES:\n",
    "                with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    entities = []\n",
    "                    for i, line in enumerate(f):\n",
    "                        # in debug, only index 100 entities\n",
    "                        if self.DEBUG and i > 100:\n",
    "                            break\n",
    "\n",
    "                        if i == 0:\n",
    "                            continue\n",
    "\n",
    "                        if file == SHORT_ABSTRACT_PATH:\n",
    "                            abstract = self.parse_abstracts(line)\n",
    "                            if abstract is None: continue\n",
    "                            data[abstract[\"_id\"]] = abstract\n",
    "                        #    entities.append(self.parse_abstracts(line))\n",
    "                        elif file == INSTANCE_TYPES_EN_PATH:\n",
    "                            types = self.parse_instance_types(line)\n",
    "                            if types is None: continue\n",
    "                            if types[\"_id\"] in data:\n",
    "                                data[types[\"_id\"]][\"_source\"][\"instance_type\"] = types[\"doc\"][\"instance_type\"]\n",
    "                            # entities.append(self.parse_instance_types(line))\n",
    "                        else:\n",
    "                            print(\"Unknown file\", file)\n",
    "                            break\n",
    "            print(\"Indexing begins...\", \"len of data\", len(data))\n",
    "            i = 0\n",
    "            entities = []\n",
    "            for entity, obj in tqdm(data.items()):\n",
    "                if self.DEBUG and i < 5:\n",
    "                    print(\"enity: \", entity, \", obj: \", obj, \"\\n\")\n",
    "                if self.DEBUG and i > 100:\n",
    "                    break\n",
    "                entities.append(obj)\n",
    "                i += 1\n",
    "\n",
    "            if len(entities) > 0:\n",
    "                helpers.bulk(self.es, entities, index=self._index_name, raise_on_error=False)\n",
    "                entities = []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e, traceback.format_exc())\n",
    "        finally:\n",
    "            print(\"Indexing finished, time elapsed: {:4f} seconds\".format(time.time() - start_time))\n",
    "            entities = []  # reset data\n",
    "        \n",
    "    \n",
    "    \n",
    "    # factory method for creating dbpedia collection\n",
    "    @classmethod\n",
    "    def create_dbpedia_collection(cls):\n",
    "        return cls(INDEX_NAME, INDEX_SETTINGS, stop_words=stopwords.words('english'), DEBUG=DEBUG, FILES=[SHORT_ABSTRACT_PATH, INSTANCE_TYPES_EN_PATH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12735/217514096.py:54: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if self.es.indices.exists(index=self._index_name):\n",
      "/tmp/ipykernel_12735/217514096.py:56: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.es.indices.delete(index=self._index_name)\n",
      "/tmp/ipykernel_12735/217514096.py:57: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  self.es.indices.create(index=self._index_name, body=self._index_settings)\n",
      "/tmp/ipykernel_12735/217514096.py:57: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.es.indices.create(index=self._index_name, body=self._index_settings)\n"
     ]
    }
   ],
   "source": [
    "dbpedia_index = DbPediaCollection.create_dbpedia_collection()\n",
    "dbpedia_index.create_index(recreate_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing begins... len of data 4926000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4926000/4926000 [00:02<00:00, 2252690.30it/s]\n",
      "/tmp/ipykernel_12735/217514096.py:117: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  helpers.bulk(self.es, entities, index=self._index_name, raise_on_error=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing finished, time elapsed: 1365.588029 seconds\n"
     ]
    }
   ],
   "source": [
    "dbpedia_index.index(bulk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4925999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12735/2253349861.py:1: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  print(\"Number of documents:\", dbpedia_index.es.count(index=INDEX_NAME)[\"count\"])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", dbpedia_index.es.count(index=INDEX_NAME)[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12735/2768235752.py:1: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  dbpedia_index.es.get(index=INDEX_NAME, id=\"animalia book\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'smart_index', '_type': '_doc', '_id': 'animalia book', '_version': 1, '_seq_no': 0, '_primary_term': 1, 'found': True, '_source': {'abstract': 'animalia illustrated childrens book graeme base originally published 1986 followed tenth anniversary edition 1996 25th anniversary edition 2012 three million copies sold special numbered signed anniversary edition also published 1996 embossed gold jacket', 'instance_type': 'book'}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_index.es.get(index=INDEX_NAME, id=\"animalia book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12735/4254249232.py:1: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  dbpedia_index.es.get(index=INDEX_NAME, id=\"actrius\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'smart_index', '_type': '_doc', '_id': 'actrius', '_version': 1, '_seq_no': 1, '_primary_term': 1, 'found': True, '_source': {'abstract': 'actresses catalan actrius 1997 catalan language spanish drama film produced directed ventura pons based awardwinning stage play er josep maria benet jornet film male actors roles played females film produced 1996', 'instance_type': 'film'}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_index.es.get(index=INDEX_NAME, id=\"actrius\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('DAT640-FinalProject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a85ede89b8b87632f57146d9f8a348b533f0abc2abbbf71b5bec859c7399448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
