{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import elasticsearch\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# path variables, etc.\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9796/208943912.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.info()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'yohannes-vm', 'cluster_name': 'elasticsearch', 'cluster_uuid': '1HOaRd2USZ6xr57dnRGoig', 'version': {'number': '7.17.7', 'build_flavor': 'default', 'build_type': 'deb', 'build_hash': '78dcaaa8cee33438b91eca7f5c7f56a70fec9e80', 'build_date': '2022-10-17T15:29:54.167373105Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line, remove_stopwords=False):\n",
    "    line = line.strip().lower().replace(\"_\", \" \").translate(str.maketrans('', '', string.punctuation))\n",
    "    return \" \".join([\n",
    "        term \n",
    "        for term in re.sub(r\"\\s+\", \" \", line).split(\" \") \n",
    "        if term not in stop_words\n",
    "    ]).strip() if remove_stopwords else line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test queries\n",
    "def load_queries(train_query_path: str, test_query_path: str) -> Tuple[Dict]:\n",
    "    files = [train_query_path, test_query_path]\n",
    "    train_queries, test_queries = {}, {}\n",
    "\n",
    "    for file in files:\n",
    "        print(f\"Loading queries from {file}\")\n",
    "        with open(file, \"r\") as f:\n",
    "            try:\n",
    "                data = json.loads(f.read())\n",
    "                for query in data:\n",
    "                    query_id, query_question = query.get('id', '').lower(), query.get('question', '')\n",
    "\n",
    "                    # some of the questions are null, check for that\n",
    "                    if not query_question: continue\n",
    "\n",
    "                    if file == train_query_path:\n",
    "                        query_category, query_type = query.get('category', '').lower(), ' '.join(query.get('type', [])).replace(\"dbo:\", \"\").lower()\n",
    "                        if query_category != 'resource': continue #only consider resource queries\n",
    "                        train_queries[query_id] = {\"query\": preprocess(query_question.lower()), \"category\": query_category, \"type\": query_type}\n",
    "                    elif file == test_query_path:\n",
    "                        test_queries[query_id] = {\"query\": preprocess(query_question.lower())}\n",
    "            except Exception as e:\n",
    "                print(e, query)\n",
    "    return train_queries, test_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading queries from /mntnvme/datasets/DBpedia/smarttask_dbpedia_train.json\n",
      "Loading queries from /mntnvme/datasets/DBpedia/smarttask_dbpedia_test_questions.json\n",
      "Train queries:  9573 Test queries:  4369 \n",
      "\n",
      "Sample Train query:  {'query': 'what are some bands out to texarkana', 'category': 'resource', 'type': 'band group organisation agent'}\n",
      "Sample Test query:  {'query': 'what is the newspaper with the max publication interval'}\n"
     ]
    }
   ],
   "source": [
    "train_queries, test_queries = load_queries(QUERY_TRAIN_PATH, QUERY_TEST_PATH)\n",
    "print(\"Train queries: \", len(train_queries), \"Test queries: \", len(test_queries), \"\\n\")\n",
    "\n",
    "print(\"Sample Train query: \", train_queries['dbpedia_21160'])\n",
    "print(\"Sample Test query: \", test_queries['dbpedia_14677'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es: Elasticsearch, query: str, index_name: str = INDEX_NAME):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index.\n",
    "       Ref: Assigment 3 - DAT640\n",
    "\n",
    "    Args:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        index_name: Name of the index with respect to which the query is analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in abstracts.\n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index_name, body={\"text\": query})[\"tokens\"]\n",
    "    query_terms = []\n",
    "\n",
    "    for token in sorted(tokens, key=lambda x: x[\"position\"]):\n",
    "        hits = es.search(index=index_name,\n",
    "                         body={'query': {'match': {'abstract': token['token']}}},\n",
    "                         _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None: continue\n",
    "        query_terms.append(token['token'])\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9573 [00:00<?, ?it/s]/tmp/ipykernel_9796/4080730600.py:13: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index=index_name, body={\"text\": query})[\"tokens\"]\n",
      "/tmp/ipykernel_9796/4080730600.py:13: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  tokens = es.indices.analyze(index=index_name, body={\"text\": query})[\"tokens\"]\n",
      "/tmp/ipykernel_9796/4080730600.py:17: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  hits = es.search(index=index_name,\n",
      "/tmp/ipykernel_9796/4080730600.py:17: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  hits = es.search(index=index_name,\n",
      "100%|██████████| 9573/9573 [36:44<00:00,  4.34it/s]  \n",
      "100%|██████████| 4369/4369 [15:48<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for query_id, query in tqdm(train_queries.items()): \n",
    "    train_queries[query_id]['query_terms'] = analyze_query(es, query['query'])\n",
    "\n",
    "for query_id, query in tqdm(test_queries.items()):\n",
    "    test_queries[query_id]['query_terms'] = analyze_query(es, query['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train query:  {'query': 'what are some bands out to texarkana', 'category': 'resource', 'type': 'band group organisation agent', 'query_terms': ['what', 'some', 'bands', 'out', 'texarkana']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Train query: \", train_queries['dbpedia_21160'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(es, train_queries, index_name=INDEX_NAME, k=10):\n",
    "    # evaluate retrieval\n",
    "    train_retrieval_results = {}\n",
    "    hit_count = 0\n",
    "    for query_id, query_values in tqdm(train_queries.items()):\n",
    "        try:\n",
    "            hits = es.search(index=index_name, _source=True, \n",
    "                    body={\"query\": \n",
    "                            { \"bool\": {\"must\": {\"match\": {\"abstract\": \" \".join(query_values[\"query_terms\"])}},\n",
    "                                      \"must_not\": {\"match\": {\"instance_type\": \"_\"}}}\n",
    "                            }}, size=k)[\"hits\"][\"hits\"]\n",
    "            hit_count += len(hits)\n",
    "            train_retrieval_results[query_id] = [hit['_source']['instance_type'] for hit in hits]\n",
    "        except Exception as e:\n",
    "            print(e, query_values)\n",
    "    \n",
    "    return train_retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9573 [00:00<?, ?it/s]/tmp/ipykernel_9796/3803168678.py:7: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  hits = es.search(index=index_name, _source=True,\n",
      "/tmp/ipykernel_9796/3803168678.py:7: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  hits = es.search(index=index_name, _source=True,\n",
      "100%|██████████| 9573/9573 [05:27<00:00, 29.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train retrieval results:  9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_retrieval_results = evaluate_retrieval(es, train_queries)\n",
    "print(\"Train retrieval results: \", len(train_retrieval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbpedia_14427 ['thing', 'film', 'film', 'album', 'holiday', 'musical', 'televisionepisode', 'album', 'album']\n",
      "dbpedia_3681 ['book', 'televisionshow', 'thing', 'film', 'person', 'film', 'person', 'televisionshow', 'film', 'film']\n",
      "dbpedia_12020 ['thing', 'country', 'administrativeregion', '_', 'thing', 'administrativeregion', 'legislature', '_', '_']\n",
      "dbpedia_10315 ['_', '_', '_', '_', '_', 'thing', '_', '_', 'thing']\n",
      "dbpedia_1335 ['person', 'officeholder', '_', '_', 'film', 'officeholder', 'person', 'film', '_']\n",
      "dbpedia_6016 ['_', '_', '_', '_', '_', '_', 'thing', '_', '_']\n",
      "dbpedia_3432 ['thing', 'thing', '_', '_', 'film', '_', 'saint', '_', 'thing']\n",
      "dbpedia_16006 ['officeholder', '_', 'station', 'crater', '_', 'thing', 'person', '_', 'book']\n",
      "dbpedia_278 ['person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'award']\n",
      "dbpedia_7661 ['soccerplayer', 'soccerplayer', 'thing', 'soccerplayer', 'soccerplayer', 'thing', 'soccermanager', '_', 'governor']\n"
     ]
    }
   ],
   "source": [
    "for query_id, results in list(train_retrieval_results.items())[:10]:\n",
    "    print(query_id, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code from [this](https://github.com/BerntA/IR-SMART) repo to do the evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mntnvme/datasets/DBpedia/dbpedia_types.tsv\n"
     ]
    }
   ],
   "source": [
    "print(DBPEDIA_TYPES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDBPediaTypes():\n",
    "    kv = {}\n",
    "    max_depth = 0\n",
    "    with open(DBPEDIA_TYPES_PATH, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0: # Skip header\n",
    "                continue\n",
    "            line = line.strip().lower().split('\\t')\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            type_name, depth, parent_type = line[0].split(':')[-1], int(line[1]), line[-1].split(':')[-1]\n",
    "            if (len(type_name) == 0) or (len(parent_type) == 0):\n",
    "                continue\n",
    "            kv[type_name] = {'depth':depth, 'parent':parent_type}\n",
    "            max_depth = max(depth, max_depth)\n",
    "    return kv, max_depth\n",
    "\n",
    "type_hierarchy, max_depth = loadDBPediaTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTypeHierarchy(kv, items, target):\n",
    "    if not target in kv:\n",
    "        return\n",
    "    items.append(target)\n",
    "    getTypeHierarchy(kv, items, kv[target]['parent'])\n",
    "\n",
    "def buildDBPediaTypeHierarchy(kv, target, reverse=True):\n",
    "    items = [] # List of types, representing the hierarchy of the types related to the target.\n",
    "    getTypeHierarchy(kv, items, target)\n",
    "    if reverse:\n",
    "        return items[::-1] # Reverse the order to return the correct hierarchy where the first item = top level.\n",
    "    return items\n",
    "\n",
    "def cacheDBPediaPaths():\n",
    "    \"\"\"Simplify Evaluation Path Computations\"\"\"\n",
    "    for k in type_hierarchy.keys():\n",
    "        type_hierarchy[k]['path'] = buildDBPediaTypeHierarchy(type_hierarchy, k, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'writtenwork', 'comic']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildDBPediaTypeHierarchy(type_hierarchy, 'comic') # Example hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dcg(gains, k=5):\n",
    "    \"\"\"\n",
    "    Computes DCG for a given ranking.\n",
    "    Traditional DCG formula: DCG_k = sum_{i=1}^k gain_i / log_2(i+1).\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(0, min(k, len(gains))):\n",
    "        dcg += gains[i] / math.log(i + 2, 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg(gains, ideal_gains, k=5):\n",
    "    \"\"\"Computes NDCG given gains for a ranking as well as the ideal gains.\"\"\"\n",
    "    try:\n",
    "        return dcg(gains, k) / dcg(ideal_gains, k)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_type_path(type, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Gets the type's path in the hierarchy (excluding the root type, like owl:Thing).\n",
    "    The path for each type is computed only once then cached in type_hierarchy,\n",
    "    to save computation.\n",
    "    \"\"\"\n",
    "    if not type in type_hierarchy:\n",
    "        type_hierarchy[type] = {'depth':1, 'parent':'', 'path':[type]}\n",
    "    if \"path\" in type_hierarchy[type]:\n",
    "        return type_hierarchy[type]['path']\n",
    "    return []\n",
    "\n",
    "def get_type_distance(type1, type2, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Computes the distance between two types in the hierarchy.\n",
    "    Distance is defined to be the number of steps between them in the hierarchy,\n",
    "    if they lie on the same path (which is 0 if the two types match), and\n",
    "    infinity otherwise.\n",
    "    \"\"\"\n",
    "    type1_path = get_type_path(type1, type_hierarchy)\n",
    "    type2_path = get_type_path(type2, type_hierarchy)\n",
    "    distance = math.inf\n",
    "    if type1 in type2_path:\n",
    "        distance = type2_path.index(type1)\n",
    "    if type2 in type1_path:\n",
    "        distance = min(type1_path.index(type2), distance)\n",
    "    return distance\n",
    "\n",
    "def get_most_specific_types(types, type_hierarchy):\n",
    "    \"\"\"Filters a set of input types to most specific types w.r.t the type\n",
    "    hierarchy; i.e., super-types are removed.\"\"\"\n",
    "    filtered_types = set(types)\n",
    "    for type in types:\n",
    "        type_path = get_type_path(type, type_hierarchy)\n",
    "        for supertype in type_path[1:]:\n",
    "            if supertype in filtered_types:\n",
    "                filtered_types.remove(supertype)\n",
    "    return filtered_types\n",
    "\n",
    "def get_expanded_types(types, type_hierarchy):\n",
    "    \"\"\"Expands a set of types with both more specific and more generic types\n",
    "    (i.e., all super-types and sub-types).\"\"\"\n",
    "    expanded_types = set()\n",
    "    for type in types:\n",
    "        # Adding all supertypes.\n",
    "        expanded_types.update(get_type_path(type, type_hierarchy))\n",
    "        # Adding all subtypes (NOTE: this bit could be done more efficiently).\n",
    "        for type2 in type_hierarchy:\n",
    "            if type_hierarchy[type2]['depth'] <= type_hierarchy[type]['depth']:\n",
    "                continue\n",
    "            type2_path = get_type_path(type2, type_hierarchy)\n",
    "            if type in type2_path:\n",
    "                expanded_types.update(type2_path)\n",
    "    return expanded_types\n",
    "\n",
    "def compute_type_gains(predicted_types, gold_types, type_hierarchy, max_depth):\n",
    "    \"\"\"Computes gains for a ranked list of type predictions.\n",
    "\n",
    "    Following the definition of Linear gain in (Balog and Neumayer, CIKM'12),\n",
    "    the gain for a given predicted type is 0 if it is not on the same path with\n",
    "    any of the gold types, and otherwise it's $1-d(t,t_q)/h$ where $d(t,t_q)$ is\n",
    "    the distance between the predicted type and the closest matching gold type\n",
    "    in the type hierarchy and h is the maximum depth of the type hierarchy.\n",
    "\n",
    "    Args:\n",
    "        predicted_types: Ranked list of predicted types.\n",
    "        gold_types: List/set of gold types (i.e., perfect answers).\n",
    "        type_hierarchy: Dict with type hierarchy.\n",
    "        max_depth: Maximum depth of the type hierarchy.\n",
    "\n",
    "    Returns:\n",
    "        List with gain values corresponding to each item in predicted_types.\n",
    "    \"\"\"\n",
    "    gains = []\n",
    "    expanded_gold_types = get_expanded_types(gold_types, type_hierarchy)\n",
    "    for predicted_type in predicted_types:\n",
    "        if predicted_type in expanded_gold_types:\n",
    "            # Since not all gold types may lie on the same branch, we take the\n",
    "            # closest gold type for determining distance.\n",
    "            min_distance = math.inf\n",
    "            for gold_type in gold_types:\n",
    "                min_distance = min(get_type_distance(predicted_type, gold_type,\n",
    "                                                     type_hierarchy),\n",
    "                                   min_distance)\n",
    "            gains.append(1 - min_distance / max_depth)\n",
    "        else:\n",
    "            gains.append(0)\n",
    "    return gains\n",
    "\n",
    "def evaluate(result):\n",
    "    \"\"\"\n",
    "    Evaluate the resulting dictionary, compute accuracy, strict and fuzzy ndcg_5, ndcg_10 where\n",
    "    ndcg_5 and ndcg_10 is computed using lenient NDCG@k with a Linear decay.\n",
    "    \n",
    "    Arguments:\n",
    "        result: A dictionary with queryIDs: List of retrieved types from the top 10 docs, \n",
    "        and a bool indicating if there was a perfect match.\n",
    "    \"\"\"\n",
    "    accuracy = []\n",
    "    strict_ndcg_5, strict_ndcg_10 = [], []\n",
    "    for qId, obj in train_queries.items():\n",
    "        if qId not in result:\n",
    "            continue\n",
    "\n",
    "        qTypes = obj['type'].split(' ')\n",
    "        if len(qTypes) == 0:\n",
    "            continue\n",
    "\n",
    "        predicted_type = result[qId]\n",
    "        predicted_type_strict = [(1 if (t in obj['type']) else 0) for t in predicted_type]        \n",
    "        exact_match = max(predicted_type_strict + [0])\n",
    "        \n",
    "        # Filters obj types to most specific ones in the hierarchy.\n",
    "        obj_types = get_most_specific_types(qTypes, type_hierarchy)\n",
    "        gains = compute_type_gains(predicted_type, obj_types, type_hierarchy, max_depth)\n",
    "        ideal_gains = sorted(gains, reverse=True)\n",
    "\n",
    "        accuracy.append(exact_match)\n",
    "        \n",
    "        strict_ndcg_5.append(ndcg(predicted_type_strict, sorted(predicted_type_strict, reverse=True), k=5))\n",
    "        strict_ndcg_10.append(ndcg(predicted_type_strict, sorted(predicted_type_strict, reverse=True), k=10))\n",
    "        \n",
    "    print('Evaluation results (based on {} questions):'.format(len(accuracy)))\n",
    "    print('-------------------')\n",
    "    \n",
    "    print('Exact Type Prediction')\n",
    "    print('  Accuracy: {:5.3f}'.format(sum(accuracy) / len(accuracy)))\n",
    "    \n",
    "    print('Strict Type ranking')\n",
    "    print('  NDCG@5:  {:5.3f}'.format(sum(strict_ndcg_5) / len(strict_ndcg_5)))\n",
    "    print('  NDCG@10: {:5.3f}'.format(sum(strict_ndcg_10) / len(strict_ndcg_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = os.path.join(DATASETS_PATH, 'results')\n",
    "isExist = os.path.exists(RESULT_PATH)\n",
    "if not isExist:\n",
    "   os.makedirs(RESULT_PATH)\n",
    "\n",
    "def write_result_to_file(res, file):\n",
    "    \n",
    "    with open(os.path.join(RESULT_PATH, f'{file}.csv'), 'w') as f:\n",
    "        for qId, obj in res.items():\n",
    "            f.write('{},{}\\n'.format(qId, ' '.join(obj)))\n",
    "\n",
    "def read_result_from_file(file):\n",
    "    result = {}\n",
    "    with open(os.path.join(RESULT_PATH, f'{file}.csv'), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            result[line[0]] = [v for v in line[-1].split(' ') if len(v) > 0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result_to_file(train_retrieval_results, 'BM25_baseline')\n",
    "res = read_result_from_file(\"BM25_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (based on 9573 questions):\n",
      "-------------------\n",
      "Exact Type Prediction\n",
      "  Accuracy: 0.361\n",
      "Strict Type ranking\n",
      "  NDCG@5:  0.177\n",
      "  NDCG@10: 0.233\n"
     ]
    }
   ],
   "source": [
    "evaluate(train_retrieval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('DAT640-FinalProject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a85ede89b8b87632f57146d9f8a348b533f0abc2abbbf71b5bec859c7399448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
